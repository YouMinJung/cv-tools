{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "image_root = '/mnt/hd/vsr_dataset/into_thin_air_all_frames/frames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_text = f'{image_root}/*.jpg'\n",
    "files = glob.glob(reg_text)\n",
    "# files = sorted(files)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listdir과 glob를 혼합해서 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "data_path = '/mnt/hd/drone_dataset/labels'\n",
    "\n",
    "sub_dir = ''\n",
    "# 하위 폴더의 모든 파일 목록 가져오기\n",
    "reg_str = os.path.join(data_path, sub_dir, '**/*.*')\n",
    "# print(reg_str)\n",
    "files = glob.glob(reg_str, recursive=True)\n",
    "files = [file for file in files if file.endswith('.txt')]\n",
    "print(f'# files: {len(files)}' )\n",
    "\n",
    "# 하위 폴더 목록 가져오기\n",
    "dirs = os.listdir(data_path)\n",
    "dirs = [d for d in dirs if os.path.isdir(os.path.join(data_path, d))]\n",
    "\n",
    "\n",
    "def count_files(path):\n",
    "    # 하위 폴더의 모든 파일을 가져 온다.\n",
    "    reg_str = os.path.join(path, '**/*.*')\n",
    "\n",
    "    files = glob.glob(reg_str, recursive=True)\n",
    "    files = [file for file in files if file.endswith('.txt')]\n",
    "    \n",
    "    # 하위 폴더 목록 가져오기\n",
    "    dirs = os.listdir(data_path)\n",
    "    dirs = [d for d in dirs if os.path.isdir(os.path.join(data_path, d))]\n",
    "    \n",
    "    for sub_dir in dirs:\n",
    "        sub_path = os.path.join(path, sub_dir)\n",
    "        # print(os.path.join(path, sub_dir))\n",
    "    \n",
    "    print(reg_str, len(files))\n",
    "\n",
    "\n",
    "# count_files(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files[1]\n",
    "sub_files = files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_idx = {}\n",
    "for file in sub_files:\n",
    "\n",
    "    # file = files[1]\n",
    "    file_path = file.rsplit(data_path, 1)[1]\n",
    "    file_path = file_path.split(os.sep)[1:-1]\n",
    "    # print(file_path, file)\n",
    "\n",
    "    idx = ''\n",
    "    for idx_str in file_path:\n",
    "        idx = idx + f'{idx_str}/'\n",
    "        if idx in file_idx:\n",
    "            file_idx[idx] = file_idx[idx] + 1\n",
    "        else:\n",
    "            file_idx[idx] = 1\n",
    "\n",
    "items = []\n",
    "for key, value in file_idx.items():\n",
    "    # print(key, value)\n",
    "    i = {key:value}\n",
    "    items.append(f'{key},{value}\\n')\n",
    "    # print(i)\n",
    "\n",
    "print(items)\n",
    "# print(file_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('datasets.txt', 'w') as txt_file:\n",
    "    txt_file.writelines(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youtube_Exploropia_Dublin_0\n",
      "youtube_Exploropia_Frank_0\n",
      "youtube_Exploropia_Istanbul_0\n",
      "youtube_Exploropia_Kharkiv_0\n",
      "youtube_Exploropia_London_0\n",
      "youtube_Exploropia_Madrid_0\n",
      "youtube_Exploropia_Rotterdam_0\n",
      "youtube_Exploropia_Singapore_0\n",
      "cctv_Seoul_BongCheonR_0\n",
      "cctv_Seoul_Cityplazza_0\n",
      "cctv_Seoul_Dobongro_0\n",
      "cctv_Seoul_GamasanSt_0\n",
      "cctv_Busan_GudeokSt_0\n",
      "dgta_DGtav_Open_0_3\n",
      "dgta_DGtav_Open_0_2\n",
      "dgta_DGtav_Open_0_1\n",
      "Test Set: 1087\n",
      "Removed Imgs: 4473\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "\n",
    "ALL_IMG_KEY = 'all,'\n",
    "IMG_EXT = 'jpg'\n",
    "\n",
    "data_path = '/mnt/hd/drone_dataset/labels_v3'\n",
    "\n",
    "\n",
    "reg_str = os.path.join(data_path, '**/*.*')\n",
    "\n",
    "# 라벨링 폴더에 하위 폴더를 포함해서 txt 파일 목록 읽기\n",
    "anno_files = glob.glob(reg_str, recursive=True)\n",
    "anno_files = [file for file in anno_files if file.endswith('.txt')]\n",
    "\n",
    "anno_group = {}\n",
    "anno_group[ALL_IMG_KEY] = 0\n",
    "img_list = {}\n",
    "img_list[ALL_IMG_KEY] = []\n",
    "test_set_candidate = {}\n",
    "remove_group = [\n",
    "    'youtube_Exploropia_Dublin_0',\n",
    "    'youtube_Exploropia_Frank_0',\n",
    "    'youtube_Exploropia_Istanbul_0',\n",
    "    'youtube_Exploropia_Kharkiv_0',\n",
    "    'youtube_Exploropia_London_0',\n",
    "    'youtube_Exploropia_Madrid_0',\n",
    "    'youtube_Exploropia_Rotterdam_0',\n",
    "    'youtube_Exploropia_Singapore_0',\n",
    "    'cctv_Seoul_BongCheonR_0',\n",
    "    'cctv_Seoul_Cityplazza_0',\n",
    "    'cctv_Seoul_Dobongro_0',\n",
    "    'cctv_Seoul_GamasanSt_0',\n",
    "    'cctv_Busan_GudeokSt_0',\n",
    "\n",
    "    # Reduntant Folders\n",
    "    'youtube_Exploropia_Dublin',\n",
    "    'youtube_Exploropia_Frank',\n",
    "    'youtube_Exploropia_Istanbul',\n",
    "    'youtube_Exploropia_Kharkiv',\n",
    "    'youtube_Exploropia_London',\n",
    "    'youtube_Exploropia_Madrid',\n",
    "    'youtube_Exploropia_Rotterdam',\n",
    "    'youtube_Exploropia_Singapore',\n",
    "    'cctv_Seoul_BongCheonR',\n",
    "    'cctv_Seoul_Cityplazza',\n",
    "    'cctv_Seoul_Dobongro',\n",
    "    'cctv_Seoul_GamasanSt',\n",
    "    'cctv_Busan_GudeokSt',\n",
    "    'youtube_SterlingT_Seoul',\n",
    "    'youtube_Exploropia_Toronto',\n",
    "    'youtube_Exploropia_Tbilisi',\n",
    "    'youtube_Exploropia_Switzerland',\n",
    "    'youtube_Exploropia_Singapore',\n",
    "    'youtube_Exploropia_Rotterdam',\n",
    "    'youtube_Exploropia_Rome',\n",
    "    'youtube_Exploropia_Munich',\n",
    "    'youtube_Exploropia_Milan',\n",
    "    'youtube_Exploropia_Mariupol',\n",
    "    'youtube_Exploropia_Madrid',\n",
    "    'youtube_Exploropia_London',\n",
    "    'youtube_Exploropia_Kharkiv',\n",
    "    'youtube_Exploropia_Istanbul',\n",
    "    'youtube_Exploropia_Frank',\n",
    "    'youtube_Exploropia_Dublin',\n",
    "    'youtube_Exploropia_Busan',\n",
    "    'youtube_Exploropia_Brazil',\n",
    "    'youtube_Exploropia_Berlin',\n",
    "    'youtube_Exploropia_Barcelona',\n",
    "    'youtube_Exploropia_Bali',\n",
    "    'youtube_DroneTravelUsa_NewYork',\n",
    "    'youtube_8KVideosHdr_Usa',\n",
    "    'youtube_8KVideosHdr_Hawaii',\n",
    "    'youtube_8KVideosHdr_Eu',\n",
    "    'youtube_8KVideosHdr_Canada',\n",
    "    'dgta_DGtav_Open',\n",
    "    'dgta_DGtav_Open_0_3',\n",
    "    'dgta_DGtav_Open_0_2',\n",
    "    'dgta_DGtav_Open_0_1',\n",
    "    'cctv_Seoul_GangNam',\n",
    "    'cctv_Seoul_GamasanSt',\n",
    "    'cctv_Seoul_DosanStR',\n",
    "    'cctv_Seoul_Dobongro', \n",
    "    'cctv_Seoul_Cityplazza',\n",
    "    'cctv_Seoul_BongCheonR',\n",
    "    'cctv_Seongnam_PankyoSt',\n",
    "    'cctv_Busan_Jangjeon',\n",
    "    'youtube_SterlingT_Yongin',\n",
    "    'youtube_SterlingT_Suwon',\n",
    "    'cctv_Busan_DaeGyuro',\n",
    "    ]\n",
    "\n",
    "for anno_file in anno_files:\n",
    "    # 파일 절대 경로에서 상대 경로로 변경(데이터 루트)\n",
    "    anno_file = anno_file.rsplit(data_path+os.sep, 1)[1]\n",
    "    # 하위 폴더 목록 생성\n",
    "    file_path = anno_file.split(os.sep)[:-1]\n",
    "    train_set_file = f'{anno_file.rsplit(\".\", 1)[0]}.{IMG_EXT}\\n'\n",
    "\n",
    "    # 테스트 세트 후보 목록 만들기\n",
    "    test_candidate_key = '_'.join(file_path)\n",
    "    if test_candidate_key not in test_set_candidate:\n",
    "        test_set_candidate[test_candidate_key] = []\n",
    "    test_set_candidate[test_candidate_key].append(train_set_file)\n",
    "\n",
    "    path_key = ''\n",
    "\n",
    "    # 하위 폴더별 이미지 목록 만들기\n",
    "    for sub_dir in file_path:\n",
    "        path_key += f'{sub_dir},'\n",
    "        if path_key in anno_group:\n",
    "            anno_group[path_key] += 1\n",
    "        else:\n",
    "            anno_group[path_key] = 1\n",
    "            img_list[path_key] = []\n",
    "        img_list[path_key].append(train_set_file)\n",
    "\n",
    "        anno_group[ALL_IMG_KEY] += 1\n",
    "        img_list[ALL_IMG_KEY].append(train_set_file)\n",
    "\n",
    "\n",
    "# 삭제 목록에 있는 이미지 파일은 제거\n",
    "removed_imgs = []\n",
    "for group_key in remove_group:\n",
    "    if group_key in test_set_candidate:\n",
    "        removed_imgs.extend(test_set_candidate.pop(group_key))\n",
    "        print(group_key)\n",
    "    img_key = group_key.replace('_', ',')+','\n",
    "    if img_key in img_list:\n",
    "        img_list.pop(img_key)\n",
    "\n",
    "\n",
    "# 그룹별 데이터 세트 파일 생성\n",
    "datasets = []\n",
    "group_depth = 4\n",
    "for key, value in anno_group.items():\n",
    "    group_idx = key.split(',')\n",
    "    datasets.append(f'{key}'+','*(group_depth-len(group_idx[:-1]))+f'{value}\\n')\n",
    "\n",
    "with open('datasets.csv', 'w', encoding='utf-8') as txt_file:\n",
    "    txt_file.writelines(datasets)\n",
    "\n",
    "# 테스트 데이터 목록 생성하기\n",
    "test_datasets = []\n",
    "test_ratio = 0.1\n",
    "for key, value in test_set_candidate.items():\n",
    "    file_count = len(value)\n",
    "    for _ in range(3):\n",
    "        random.shuffle(value)\n",
    "    test_datasets.extend(value[:int(file_count * test_ratio)])\n",
    "\n",
    "removed_imgs.extend(test_datasets)\n",
    "print(f'Test Set: {len(test_datasets)}')\n",
    "print(f'Removed Imgs: {len(removed_imgs)}')\n",
    "\n",
    "\n",
    "with open('test.txt', \"w\") as test_file:\n",
    "    test_file.writelines(test_datasets)\n",
    "\n",
    "\n",
    "# 학습 데이터 목록 생성하기 (테스트 데이터는 제외하기)\n",
    "out_path = 'results'\n",
    "for key, value in img_list.items():\n",
    "    imgs = []\n",
    "    for img_file in value:\n",
    "        if img_file in removed_imgs:\n",
    "            continue\n",
    "        imgs.append(img_file)\n",
    "\n",
    "    train_set_file = f'{out_path}{os.sep}{key.replace(\",\", \"_\")[:-1]}.txt'\n",
    "    with open(train_set_file, \"w\", encoding='utf-8') as txt_file:\n",
    "        txt_file.writelines(imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10622 1044\n"
     ]
    }
   ],
   "source": [
    "img_file = anno_files[2].rsplit('.',1)[0] + '.png'\n",
    "img_file.rsplit(data_path+os.sep, 1)[1]\n",
    "\n",
    "print(len(anno_files), len(test_datasets))\n",
    "\n",
    "# f'{test_file.rsplit(data_path+os.sep, 1)[1]}\\n'\n",
    "\n",
    "# print(data_path)\n",
    "# print(anno_files[2].rsplit(data_path, 1)[1])\n",
    "# file_path = anno_file.rsplit(data_path, 1)[1]\n",
    "# file_path = file_path.split(os.sep)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(datasets))\n",
    "print(len(test_datasets))\n",
    "print(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test_datasets = []\n",
    "test_ratio = 0.1\n",
    "for key, value in test_set_candidate.items():\n",
    "    file_count = len(value)\n",
    "    for _ in range(3):\n",
    "        random.shuffle(value)\n",
    "    test_datasets.extend(value[:int(file_count * test_ratio)])\n",
    "\n",
    "print(len(test_datasets))\n",
    "print(test_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "b=4\n",
    "test = f'{a}'+'/'*4+f'{b}'\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in anno_files:\n",
    "    file_path = file.rsplit(data_path, 1)[1]\n",
    "    file_path = file_path.split(os.sep)[1:-1]\n",
    "    print('_'.join(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = 'results'\n",
    "for key, value in img_list.items():\n",
    "    file_name = f'{out_path}{os.sep}{key.replace(\",\", \"_\")[:-1]}.txt'\n",
    "    with open(file_name, \"w\", encoding='utf-8') as txt_file:\n",
    "        files = [f'{file}\\n' for file in value]\n",
    "        txt_file.writelines(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list\n",
    "for key in img_list.keys():\n",
    "    print(key, len(img_list[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"W:\\\\Projects\\\\test.txt\"\n",
    "with open(file_name, \"w\") as fd:\n",
    "    print(\"This is test\",file=fd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4b46364083692397d4fa1090eed67c0c7e4003ff13a64b887f838c3a186ba19"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
